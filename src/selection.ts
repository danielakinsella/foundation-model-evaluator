/**
 * Model Selection Strategy Generator
 *
 * This script analyzes evaluation results from multiple foundation models and
 * generates an optimal model selection strategy. It's designed to help choose
 * the best model for production use based on quality (similarity to ground truth)
 * and performance (latency).
 *
 * ## How It Works
 *
 * 1. **Load Results**: Reads evaluation data from `model_evaluation_results.csv`
 *    (generated by `eval-framework.ts`)
 *
 * 2. **Aggregate Metrics**: Groups results by model and calculates averages:
 *    - Average latency (response time in seconds)
 *    - Average similarity score (0-1, how close output matches ground truth)
 *
 * 3. **Normalize Scores**: Converts latency to a 0-1 score where lower latency = higher score
 *
 * 4. **Calculate Overall Score**: Weighted combination of quality and speed:
 *    - 70% weight on similarity (quality)
 *    - 30% weight on latency (performance)
 *
 * 5. **Rank Models**: Sorts by overall score to determine primary and fallback models
 *
 * 6. **Output Strategy**: Saves `model_selection_strategy.json` for use with AWS AppConfig
 *    or other configuration systems
 *
 * ## Usage
 *
 * ```bash
 * # First, run the evaluation framework to generate results
 * npx ts-node eval-framework.ts
 *
 * # Then, generate the selection strategy
 * npx ts-node selection.ts
 * ```
 *
 * ## Output Format
 *
 * ```json
 * {
 *   "primary_model": "amazon.nova-lite-v1:0",
 *   "fallback_models": ["amazon.titan-text-express-v1"],
 *   "model_scores": [...]
 * }
 * ```
 */

import * as fs from "fs";
import * as path from "path";
import { EvaluationResult, ModelSelectionStrategy, ModelScore } from "./types";

// Resolve config directory path relative to this file
const CONFIG_DIR = path.resolve(__dirname, "..", "config");



/**
 * Analyzes evaluation results and creates a model selection strategy.
 *
 * The algorithm:
 * 1. Groups all test results by model ID
 * 2. Calculates average latency and similarity score per model
 * 3. Normalizes latency to a 0-1 scale (inverted: lower latency = higher score)
 * 4. Computes weighted overall score: 70% similarity + 30% latency
 * 5. Ranks models by overall score
 *
 * @param results - Array of evaluation results from eval-framework.ts
 * @returns Strategy object with primary model, fallbacks, and detailed scores
 */
function createModelSelectionStrategy(
  results: EvaluationResult[]
): ModelSelectionStrategy {
  // Group results by model_id and calculate averages
  const modelGroups = results.reduce(
    (acc, result) => {
      if (!acc[result.model_id]) {
        acc[result.model_id] = { latencies: [], similarities: [] };
      }
      acc[result.model_id].latencies.push(result.latency);
      if (result.similarity_score !== undefined) {
        acc[result.model_id].similarities.push(result.similarity_score);
      }
      return acc;
    },
    {} as Record<string, { latencies: number[]; similarities: number[] }>
  );

  // Calculate mean latency and similarity for each model
  const modelScores: ModelScore[] = Object.entries(modelGroups).map(
    ([modelId, data]) => {
      const avgLatency =
        data.latencies.reduce((sum, v) => sum + v, 0) / data.latencies.length;
      const avgSimilarity =
        data.similarities.length > 0
          ? data.similarities.reduce((sum, v) => sum + v, 0) /
            data.similarities.length
          : 0;

      return {
        model_id: modelId,
        latency: avgLatency,
        similarity_score: avgSimilarity,
        latency_score: 0, // Will be calculated after we know max
        overall_score: 0,
      };
    }
  );

  // Normalize latency scores (lower latency is better)
  const maxLatency = Math.max(...modelScores.map((m) => m.latency));
  for (const score of modelScores) {
    score.latency_score = maxLatency > 0 ? 1 - score.latency / maxLatency : 0;
  }

  // Calculate weighted overall score (adjust weights based on priorities)
  for (const score of modelScores) {
    score.overall_score =
      0.7 * score.similarity_score + 0.3 * score.latency_score;
  }

  // Sort by overall score (descending)
  modelScores.sort((a, b) => b.overall_score - a.overall_score);

  // Create strategy
  const strategy: ModelSelectionStrategy = {
    primary_model: modelScores[0]?.model_id ?? "",
    fallback_models: modelScores.slice(1).map((m) => m.model_id),
    model_scores: modelScores,
  };

  return strategy;
}

/**
 * Parses a CSV line handling quoted fields with commas and escaped quotes.
 * Handles fields like: value1,"field with, comma","field with ""quotes""",value4
 */
function parseCSVLine(line: string): string[] {
  const result: string[] = [];
  let current = "";
  let inQuotes = false;
  let i = 0;

  while (i < line.length) {
    const char = line[i];

    if (inQuotes) {
      if (char === '"') {
        // Check for escaped quote ("")
        if (i + 1 < line.length && line[i + 1] === '"') {
          current += '"';
          i += 2;
          continue;
        }
        // End of quoted field
        inQuotes = false;
      } else {
        current += char;
      }
    } else {
      if (char === '"') {
        inQuotes = true;
      } else if (char === ",") {
        result.push(current);
        current = "";
      } else {
        current += char;
      }
    }
    i++;
  }
  result.push(current);
  return result;
}

/**
 * Parses CSV content handling multiline quoted fields.
 * Returns array of parsed rows (each row is array of field values).
 */
function parseCSV(content: string): string[][] {
  const rows: string[][] = [];
  let currentRow = "";
  let inQuotes = false;

  for (const char of content) {
    if (char === '"') {
      inQuotes = !inQuotes;
    }
    if (char === "\n" && !inQuotes) {
      if (currentRow.trim()) {
        rows.push(parseCSVLine(currentRow));
      }
      currentRow = "";
    } else {
      currentRow += char;
    }
  }
  // Don't forget the last row
  if (currentRow.trim()) {
    rows.push(parseCSVLine(currentRow));
  }
  return rows;
}

/**
 * Parses the evaluation results CSV file into structured data.
 *
 * Expected CSV format (generated by eval-framework.ts):
 * ```
 * model_id,question,output,latency,token_count,similarity_score
 * amazon.nova-lite-v1:0,"What is...","A 401(k) is...",1.234,45,0.85
 * ```
 *
 * Handles quoted fields with commas, newlines, and escaped quotes.
 *
 * @param filePath - Path to the CSV file
 * @returns Array of parsed evaluation results
 */
function loadResultsFromCSV(filePath: string): EvaluationResult[] {
  const content = fs.readFileSync(filePath, "utf-8");
  const rows = parseCSV(content);
  if (rows.length < 2) return [];

  const headers = rows[0];
  return rows.slice(1).map((values) => {
    const result: Record<string, string | number | undefined> = {};
    headers.forEach((header, i) => {
      const value = values[i];
      if (["latency", "token_count", "similarity_score"].includes(header)) {
        result[header] = value ? parseFloat(value) : undefined;
      } else {
        result[header] = value || undefined;
      }
    });
    return result as unknown as EvaluationResult;
  });
}

// ============================================================================
// Main Execution
// ============================================================================

const resultsFile = path.join(CONFIG_DIR, "model_evaluation_results.csv");
const strategyFile = path.join(CONFIG_DIR, "model_selection_strategy.json");

if (fs.existsSync(resultsFile)) {
  const results = loadResultsFromCSV(resultsFile);
  const strategy = createModelSelectionStrategy(results);

  console.log(JSON.stringify(strategy, null, 2));

  // Save strategy to file for AppConfig
  fs.writeFileSync(strategyFile, JSON.stringify(strategy, null, 2));
  console.log(`\nStrategy saved to ${strategyFile}`);
} else {
  console.error(`Results file not found: ${resultsFile}`);
  console.error("Run eval-framework.ts first to generate evaluation results.");
}
